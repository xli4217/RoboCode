Top 10 Open-Source Code Generation Models – Comprehensive Overview

Below we document ten leading open-source Large Language Models (LLMs) specialized for code. Each entry includes key metadata (model versions, parameters, license, context length, benchmark results on code tasks, training data, fine-tuning, special code features, last update, source links, deployment needs, and notes). The information is structured for easy filtering (e.g. by license, size) as one might in a spreadsheet.

Code Llama (Meta AI)
	•	Versions & Parameters: 7B, 13B, 34B, and 70B parameter models, with variants: Base, Python-specialized, and Instruct  . The 7B, 13B, and 70B versions (but not 34B) support infilling (fill-in-the-middle) capabilities .
	•	License: Released under Meta’s LLAMA 2 Community License (a permissive custom license allowing research & commercial use with acceptance of terms) .
	•	Context Window: 16,000 tokens by default (extended up to 100k tokens through fine-tuning the positional embeddings)  , enabling repository-level inputs.
	•	Benchmark Scores: Achieved state-of-the-art among open models on release. For instance, Code Llama scored up to 67% pass@1 on HumanEval (Python) and 65% on MBPP . Notably, even the smallest 7B-Python model outperformed a 70B general LLaMA2 on these coding benchmarks . The 70B model’s 67% on HumanEval is close to OpenAI’s code-cushman (Codex) performance .
	•	Training Data: Built on LLaMA 2 foundation models, then further trained on a large code corpus (~500B tokens of permissively licensed code from GitHub, etc.) and discussion data. Python code is especially emphasized in the Code Llama–Python variant . All training code was permissively licensed (Apache-2.0, MIT, BSD) to avoid GPL contamination.
	•	Fine-Tuning Details: A multi-stage specialization: starting from LLaMA 2 pretrained on text+code, then code fine-tuning (on 2+ epochs of code data), plus an additional Python-focused pass (100B more tokens) for the Python variant, and finally an instruction-tuning phase (including self-generated unit tests/solutions) for the Instruct models   . The instruct fine-tune improves alignment and safety with only minor impact on code performance .
	•	Code-Specific Features: All Code Llama models can perform left-to-right generation; 7B, 13B, 70B (but not 34B) also support fill-in-the-middle mode for code completion inside files  . They handle long code with the extended context. They were released with examples for code completion and infilling usage.
	•	Last Updated: Released August 2023 (research paper v3 updated in Sept 2023). No further official versions as of 2024.
	•	Source URL: Meta AI’s announcement and arXiv paper ; model weights on Hugging Face under meta-llama organization.
	•	Deployment Requirements: The largest 70B model typically requires ~80GB GPU memory (e.g. 2×40GB GPUs or 1×80GB A100) for inference. The 34B model fits on ~40GB VRAM (or 2×24GB with 8-bit quantization). All Code Llama models support 16-bit and 8-bit loading (with transformer libraries) for easier deployment.
	•	Notes: Code Llama set a new standard for open code models upon release , outperforming previous open models on many benchmarks. It is capable in multiple programming languages and excels at Python. The instruct variant is safer for general use, while the base and Python variants may produce more direct code solutions.

StarCoder Family (BigCode Project)
	•	Versions & Parameters: StarCoder is a 15.5B-parameter decoder-only model released in May 2023, and its successor StarCoder2 (Feb 2024) comes in 3B, 7B, and 15B sizes . StarCoder2-15B is the flagship model.
	•	License: Uses the BigCode Open RAIL-M license, which is permissive for commercial use with some usage restrictions (e.g. not for malware generation, must follow ethical AI use cases) . This applies to both StarCoder and StarCoder2.
	•	Context Window: StarCoder (v1) supports up to 8,192 tokens context. StarCoder2 expanded this to 16,384 tokens by employing Grouped Query Attention (GQA) with a sliding window mechanism  . This long context is well-suited for handling entire files or multiple classes in one prompt.
	•	Benchmark Scores: The original StarCoder (15B) achieves about 33.6% pass@1 on HumanEval and ~43-45% on MBPP , comparable to OpenAI’s early Codex models. StarCoder2-15B significantly improved performance – it “matches or outperforms Code Llama 34B, and approaches DeepSeek-Coder 33B” on multiple coding benchmarks . For example, StarCoder2-15B reaches ~50-60% on HumanEval (exact numbers were not in the model card, but BigCode reported it nearly closing the gap to DeepSeek’s 78% ). StarCoder2’s smaller 3B model even outperforms the original StarCoder15B , showing the benefit of the larger training dataset.
	•	Training Data: StarCoder v1 was trained on The Stack (v1),  a 3.1TB dataset of source code in 80+ programming languages. It was trained on 0.5 trillion tokens of code. StarCoder2 uses the new Stack v2 (which contains ~4× more code: 900B tokens after deduplication) across 600+ languages  . Both versions exclude non-permissive licenses via filtering. Some natural language content (e.g. documentation, StackOverflow Q&A) is included to improve understanding of code in context  .
	•	Fine-Tuning Details: StarCoder v1 has a base model (trained on raw code) and an optional fine-tuned StarCoder variant for instruction-following (trained via an RLHF-like process with human feedback, known as StarCoder-Plus). StarCoder2 similarly can be instruction-tuned; an example is StarCoder2-15B-Instruct-v0.1, aligned with self-chat recipes (fully open pipeline) . These instruct versions are better at following NL prompts (e.g. “write a function to do X”).
	•	Code-Specific Features: Both StarCoder and StarCoder2 support Fill-in-the-Middle: they were trained with a special <|fim|> token allowing insertion of code in the middle of a file . This makes them capable of completing code with given prefixes and suffixes, useful for IDE integration. They also understand programming context in many languages (StarCoder2 covers 600+ languages, including niche and legacy languages)  .
	•	Last Updated: StarCoder v1 – May 2023; StarCoder2 – released February 2024 (with a technical report on Hugging Face blog)  . Ongoing BigCode community efforts may further improve these models.
	•	Source URLs: Hugging Face model repo bigcode/starcoder-15b and bigcode/starcoder2-15b (with detailed model cards). Official blog: “StarCoder2 and The Stack v2”  .
	•	Deployment Requirements: The 15B models require roughly 30–32 GB of GPU memory in 16-bit (FP16) mode . For example, StarCoder2-15B in bfloat16 occupies ~32.25 GB RAM . Using 8-bit quantization, it can run on a single 16 GB GPU (with some performance hit). Multi-GPU or high-memory accelerators (A100 40GB, etc.) are recommended for full 15B deployment. The smaller 3B and 7B StarCoder2 models are much lighter (7B ~ 14GB FP16).
	•	Notes: StarCoder was a milestone as an open-source analog of Codex, and StarCoder2 further narrowed the gap with proprietary models. StarCoder2’s transparency (training data and process fully released) and permissive license make it attractive for enterprise use. It is especially proficient at multi-language code generation: e.g. writing code in C++, Java, JS, Go, etc., not just Python . Users should note that the base models are not instruction-tuned, so prompting with proper code context or using the instruct variant is important for best results .

WizardCoder (WizardLM Team)
	•	Versions & Lineage: WizardCoder is an instruction-tuned code model introduced in mid-2023 by the WizardLM research team. The initial WizardCoder-V1.0 (15B parameters) was fine-tuned from StarCoder base (15B) using ~78k evolved instruction examples tailored to coding  . In Jan 2024, an enhanced WizardCoder-33B-V1.1 was released, built on a DeepSeek 33B base model . WizardCoder also has smaller variants (1B, 3B, 7B, 13B, etc. derived from LLaMA 2 models) and Python-focused versions for some sizes .
	•	License: Varies by version. The 15B V1.0 inherits StarCoder’s OpenRAIL-M (allowing commercial use) . However, the top-performing 33B V1.1 is under a Microsoft Research License (non-commercial)  . (It uses DeepSeek’s base, which is permissive, but apparently includes MS-contributed data or weights that restrict usage to research.) Always check the specific model card – e.g., smaller WizardCoder variants based on LLaMA2 carry Meta’s LLaMA2 license (commercial allowed with acceptance) .
	•	Context Window: Follows the base model’s context length – e.g., WizardCoder-15B (StarCoder base) has 8K context, while WizardCoder-33B (DeepSeek base) supports 16K. The 33B V1.1 was reported and evaluated with a 2048-token max for benchmarks  , but the model itself supports the full 16k of DeepSeek’s training. There is no long-context extension beyond what the base provided.
	•	Benchmark Scores: WizardCoder-15B v1.0 achieved 57.3% pass@1 on HumanEval and 51.8% on MBPP   – a huge jump (~+22 points) over previous open models in mid-2023 . This made it the state-of-the-art open model at the time. The updated WizardCoder-33B v1.1 further boosts performance to 79.9% on HumanEval and 78.9% on MBPP   – notably surpassing OpenAI’s ChatGPT-3.5 and even the DeepSeek 33B model it was built on  . These results rank WizardCoder-33B among the best coding models overall (only GPT-4 is significantly higher on HumanEval) . Smaller variants: e.g., WizardCoder-Python-34B scored 73.2% HumanEval , and the 7B/13B versions reach in the 55–64% range on HumanEval  – very strong for those sizes.
	•	Training Data: WizardCoder leveraged existing base models’ pretraining on code, then applied its own Evol-Instruct fine-tuning. The 15B model used the StarCoder training data (The Stack) as its base, and then 78K synthesized Q&A pairs for coding tasks . The 33B model starts from DeepSeek-Coder (trained on 2T tokens of mostly code, bilingual EN/ZH) , then presumably uses an instruction set (the authors mention it’s trained “from deepseek-coder-33b-base” with the Evol-Instruct pipeline) . So effectively, WizardCoder’s strength comes from high-quality instruction tuning data rather than additional raw code pretraining.
	•	Fine-Tuning Details: Evol-Instruct is the method used: the team generated diverse and progressively harder coding instructions and answers (by evolving simple prompts into more complex ones). WizardCoder-15B was fine-tuned on this set for one epoch . The 33B v1.1 fine-tune used an updated instruction dataset and possibly improvements like HumanEval test-filtering (they carefully deduplicated any overlap with evals) . No reinforcement learning was explicitly mentioned, but the process yields an instruction-following model specialized in code tasks.
	•	Code-Specific Features: WizardCoder is designed to follow natural language instructions and produce code or explanations. It is essentially a chat-oriented model (the HF repo provides a default prompt template with an Instruction: and Response: format ). It excels at writing functions given descriptions, generating unit-test-passing solutions, and explaining code. The base StarCoder provided fill-in-the-middle, but WizardCoder’s instruction tuning might slightly degrade infilling ability (focuses on prompt → completion). Still, it can do code completion in the usual left-to-right manner. WizardCoder-33B can handle both English and Chinese prompts (due to DeepSeek training data), making it versatile for bilingual developers .
	•	Last Updated: Version 1.0 was released in July 2023. Version 1.1 (33B) in January 2024 . The team also released WizardMath and others; further code model updates might appear via WizardCoder-Python series. Keep an eye on their GitHub/Twitter for new releases .
	•	Source URL: Hugging Face models under WizardLM/WizardCoder-* and the project’s GitHub (WizardLM repo). The arXiv paper “Empowering Code LLMs with Evol-Instruct”  details the approach. The model card on HF provides comparative tables  .
	•	Deployment Requirements: The 33B model is large – running it in 16-bit precision requires ~65 GB GPU memory (so typically 2×40GB GPUs or 1×80GB). The authors used 8×A100 80GB for evaluation to generate 164 problem solutions in parallel  . GPTQ or other 4-bit quantization can reduce memory to ~20GB at some cost. The 15B model is easier: ~30GB in FP16, or ~18GB with 8-bit. The smaller WizardCoder versions (7B and below) can run on a single consumer GPU (8–16 GB). In all cases, for chat applications it’s recommended to use a conversation template as provided.
	•	Notes: WizardCoder’s high accuracy on standard benchmarks is impressive, but like all models it can occasionally produce incorrect or insecure code. The team explicitly checked for test data contamination to ensure genuine generalization . Because the 33B v1.1 is non-commercial, organizations seeking to use it commercially might opt for the 34B-Python v1.0 (73% HumanEval) which is under LLaMA2 license (commercial-friendly) . Overall, WizardCoder demonstrates the power of targeted instruction tuning – it effectively “unlocked” much stronger performance from existing open models without additional pretraining data.

DeepSeek Coder (深度求索 Code LLM)
	•	Versions & Parameters: DeepSeek Coder is a family of open code-focused LLMs released in 2023 by the Chinese company DeepSeek.ai. The V1 models came in 1.3B, 5.7B, 6.7B, and 33B parameters . The flagship was DeepSeek-Coder-33B (available in base and instruct). In January 2024, DeepSeek-Coder-V2 was announced, introducing a Mixture-of-Experts architecture with a massive 236B parameters (of which 21B are active at a time) and a smaller MoE version with 16B (2.4B active) . V2’s 236B model is effectively an ensemble of experts giving performance beyond what a dense 33B could.
	•	License: DeepSeek’s code and model are open-source. The model weights license is a custom DeepSeek License that allows commercial use (similar spirit to MIT) . In fact, the repository states it supports commercial use with proper attribution. (The code in their GitHub is MIT licensed, and the model card refers to a Model License file). DeepSeek-Coder V2 weights are also released openly on Hugging Face under the same terms.
	•	Context Window: 16,384 tokens in DeepSeek Coder v1 (all model sizes) . This large window was intended for “project-level” code understanding. V2 pushes context even further to 128K tokens maximum , which is extremely high – enabling it to handle very large files or multiple files. (Such long context may require specialized inference frameworks; DeepSeek provides support via vLLM and inference code.)
	•	Benchmark Scores: DeepSeek Coder v1 claimed state-of-the-art open-source results as of late 2023. The 33B-Instruct model achieved about 78.7% pass@1 on HumanEval and 78.7% on MBPP , essentially tying WizardCoder-33B and nearing GPT-3.5 level . (WizardCoder v1.1 later slightly surpassed this .) For reference, DeepSeek 6.7B base scored ~49.4% on HumanEval . In a Chinese multi-language coding benchmark, DeepSeek outperformed other models as well . DeepSeek-Coder V2 further improves: the team reports it “achieves performance comparable to GPT-4 Turbo in code-specific tasks” . They state V2 exceeds GPT-4 Turbo, Claude 2, and Google Gemini (pro) on standard coding and math benchmarks . While exact numbers aren’t given in the card, this implies pass@1 in the 80%+ range on HumanEval (close to GPT-4’s ~85% ). These claims, if validated, make DeepSeek-Coder-V2 one of the most powerful code models available.
	•	Training Data: DeepSeek Coder v1 models were trained from scratch on 2 trillion tokens, predominantly code. The data mix was 87% code and 13% natural language (in both English and Chinese)  – indicating a huge volume of code (likely sourced from GitHub, etc.) plus some Q&A or docs. They mention “project-level code corpus” – possibly meaning they trained on whole repositories (to enable better long-range coherence) . The training included an extra fill-in-the-blank task to teach infilling capabilities . For V2, they continued pre-training from a DeepSeek-V2 intermediate model with +6 trillion tokens of code and math data . V2 also expanded supported programming languages from 86 to 338 languages (!) , basically any language found in their data. This suggests an incredibly diverse and extensive code training set.
	•	Fine-Tuning Details: The 33B Instruct variant was fine-tuned from the 33B base on 2B tokens of instruction-answer data  (likely including both human-curated and synthesized coding queries). V1 instruction tuning covered multi-language user prompts. DeepSeek V2 models are released in both base and instruct versions as well . The instruct tuning would target conversation and problem-solving format (like explaining code, debugging on request, etc.). DeepSeek emphasizes they performed rigorous data deduplication to avoid leaking test solutions in the fine-tuning phase as well .
	•	Code-Specific Features: DeepSeek Coder supports “project-level” code completion and infilling – the 16K context and training objective allow it to fill in code in the middle of a file or consider multiple files’ content . It likely has special tokens for blank filling similar to InCoder/StarCoder. The models are bilingual, meaning they understand both English and Chinese instructions about code (and possibly code comments/docstrings in either language). V2’s Mixture-of-Experts design means it can scale to much larger parameter count without linear increase in runtime – only relevant “experts” are active per query . This gives it an advantage in leveraging a huge parameter space efficiently. Also, the 128K context in V2 means it can take in entire codebases for analysis (though with current hardware, very long inputs are slow and memory-intensive).
	•	Last Updated: DeepSeek Coder v1 was released in mid/late 2023 (the paper was Jan 2024 ). V2 was released in January 2024 (arXiv 2401.06066). They are likely to continue iterating (the name DeepSeek suggests an ongoing project).
	•	Source URLs: Hugging Face hub (deepseek-ai organization) hosts model cards for DeepSeek-Coder-33B-instruct  and DeepSeek-Coder-V2-Instruct (236B) . The paper “When LLM Meets Programming – The Rise of Code Intelligence” details V1 (33B) and a technical report for V2 is referenced on the HF card. Official website: deepseek.com (with a chat demo).
	•	Deployment Requirements: The 33B model is dense and requires roughly 60+ GB memory (in BF16). It can be run on two 48GB GPUs or four 24GB GPUs with tensor parallelism. DeepSeek V2-236B is extremely large – it “requires 80GB×8 GPUs” for BF16 inference  (i.e. 8 A100 80GB nodes). However, because it’s Mixture-of-Experts, the active parameter count is 21B, so if expert routing is optimized, runtime memory might be effectively lower (the 16B MoE variant only needs one GPU). DeepSeek provides optimized inference code (with vLLM, etc.) to handle the MoE model. For most users, using the 33B or 16B models will be far more tractable.
	•	Notes: DeepSeek’s contribution is notable for bringing bilingual and project-scale capabilities. The inclusion of Chinese language makes it particularly appealing for Chinese developers (it can understand prompts and code comments in Chinese, and possibly generate code comments in Chinese as well) . Given its high performance, DeepSeek Coder is a top choice if one needs an open model approaching GPT-4 level coding ability, especially once the non-commercial restriction (for WizardCoder 33B) is a concern – DeepSeek allows commercial use. One should keep in mind the infrastructure needed for the largest model and possibly opt for smaller scale or quantization if deploying in practice.

CodeT5 and CodeT5+ (Salesforce)
	•	Versions & Parameters: CodeT5 is a family of encoder–decoder Transformer models for code from Salesforce (initially introduced in 2021). The original CodeT5 came in sizes Small (60M), Base (220M) and Large (770M). In 2023, an improved CodeT5+ family was released, scaling up to 2B, 6B, and 16B parameters . The largest InstructCodeT5+ 16B is the top-performing variant tuned for instruction following. CodeT5 models are text-to-text (sequence-to-sequence) models, which gives them flexibility to handle tasks like code summarization or translation in addition to generation.
	•	License: The CodeT5+ 16B model weights are openly available for research. The code and checkpoints are under a permissive license (the model card doesn’t specify, but the project uses an MIT license for most components). Salesforce has indicated no tiers or restrictions on usage in open-source releases. (Thus, we assume MIT License or similar for CodeT5+ models, allowing commercial self-hosting .)
	•	Context Window: As an encoder-decoder model, the context is split between input and output. CodeT5+16B uses a T5-derived architecture; it typically supports input lengths ~512 or 1024 tokens and can generate similar lengths. It’s not as long-context as GPT-style models for single sequence, though one can feed longer code by truncation. The focus is often on method or small-file level generation.
	•	Benchmark Scores: CodeT5+ (instruction-tuned) was state-of-the-art among open models as of mid-2023 for code generation in zero-shot settings. InstructCodeT5+ 16B achieved 35.0% pass@1 and 54.5% pass@10 on HumanEval . This actually surpassed OpenAI’s code-cushman-001 (Codex 12B) on HumanEval in zero-shot mode . It was a significant result for a 16B model at the time (prior to Code Llama and WizardCoder). Smaller CodeT5+ models also showed strong performance on various tasks; for instance, 6B and 2B models outperform similar-sized GPT-J or GPT-neo on code understanding tasks (and even some math programming tasks, per the paper). That said, by late 2023, larger decoder-only models (WizardCoder, etc.) have higher pure code-gen numbers on HumanEval. CodeT5’s strength is also in versatility (code understanding benchmarks, completion tasks, etc., beyond just generation).
	•	Training Data: CodeT5+ models are trained on a mixture of permissively licensed code from GitHub (they used a filtered subset of BigCode’s GitHub data) covering 8-10 programming languages . The pretraining is multi-modal: both unimodal code and bimodal natural language-code data, enabling it to handle docstrings and explanations. The tasks used include span denoising, causal language modeling, contrastive learning, and text-code matching . For example, it learns to recover masked code spans, distinguish aligned code-comment pairs, etc. This “mixture of objectives” is designed to improve the representations for diverse code tasks . After pretraining, the largest model also had an extra phase focused on Python code (one epoch on Python subset with causal LM) to adapt better for code generation .
	•	Fine-Tuning Details: Finally, CodeT5+16B underwent instruction tuning (called InstructCodeT5+) using a dataset of NL instructions and code solutions, including the CodeAlpaca dataset (which is code-related instruction-following data) . This aligns the model to follow human prompts more naturally. The instruction tuning is fully permissive/transparent (only open data used). The result is a model that you can prompt in plain English (e.g. “Implement a function to X”) and it will output the code solution.
	•	Code-Specific Features: As an encoder-decoder, CodeT5 can be used for code-to-code transformations (e.g. code refinement, translation between languages) by feeding source code in the encoder and generating target code from the decoder. It’s also good at code understanding tasks like summarizing code or generating comments, because the bidirectional encoder can absorb the code context effectively. The model supports multiple languages: the training data included Python, Java, JavaScript, Go, PHP, Ruby, C, C++, C#, etc. . However, Python was a primary focus, and the benchmarks suggest it’s strongest in Python generation. One feature of CodeT5+16B is a “shallow encoder, deep decoder” architecture – the encoder is kept smaller and the decoder larger . This favors generation quality while still allowing encoding of input.
	•	Last Updated: The CodeT5+ models were released in May 2023 (arXiv v2) with the 16B checkpoint on Hugging Face. No newer version has been announced as of 2024. (Salesforce may integrate some of this into their product platforms, but the research release is stable.)
	•	Source URLs: Hugging Face models like Salesforce/codet5p-770m and Salesforce/instructcodet5p-16b (with model card citing the CodeT5+ paper)  . The paper “CodeT5+: Open Code LLMs for Code Understanding and Generation” provides full details  . Salesforce also posted a blog about CodeT5+ in 2023.
	•	Deployment Requirements: Being an encoder-decoder, the 16B model is split across two networks but overall memory footprint is similar to a 16B decoder-only model (~32 GB in FP16). In practice, generating code with it can be a bit slower than a decoder-only model of similar size, due to the encoder pass. It can be run on a single 32GB GPU or two 16GB GPUs (one for encoder, one for decoder, if using model parallelism). The 770M and 2B models are much lighter (2B fits in ~4GB). The Transformers library supports CodeT5 out-of-the-box; no special hardware beyond sufficient VRAM is needed.
	•	Notes: CodeT5+ stands out for its multi-task proficiency. It not only writes code, but can explain it or translate it. This makes it a good all-around coding assistant, especially in IDE plugins or scenarios where you might ask for an explanation of code as well as code generation. Its performance on pure generation (35% HumanEval) was surpassed later by larger models like Code Llama, but those models lack an encoder and thus are less suited for certain code-understanding tasks. Depending on your use case, CodeT5+ could be a better fit (for example, for a tool that needs to reason about existing code and generate a refactored version, an encoder-decoder model can be advantageous).

CodeGen (Salesforce/NVIDIA)
	•	Versions & Parameters: CodeGen is an early open LLM for code (March 2022). It is a decoder-only Transformer released in multiple sizes: 350M, 2.7B, 6B, and 16B parameters. There are variants by training data: CodeGen-Multi (trained on mixed-code & text) and CodeGen-Mono (trained only on a single language’s code, notably Python Mono for specialized performance) . The top model is CodeGen-16B.
	•	License: CodeGen was released under the Apache 2.0 License  – a fully permissive open-source license. This means individuals and enterprises can use and modify it freely (provided they include the license notice). CodeGen’s open model was a significant step at the time, as it allowed commercial use as opposed to OpenAI’s closed Codex.
	•	Context Window: 2048 tokens (standard for Transformers based on GPT-2 architecture). It was primarily intended for completing relatively short code prompts (like writing a function or a short program). There is no native support for longer contexts beyond 2k tokens in the original CodeGen releases.
	•	Benchmark Scores: On HumanEval, CodeGen’s performance depends on the variant. The CodeGen-16B-Mono (Python) model achieved about 29.3% pass@1 on HumanEval , substantially better than the multi-language 16B (which scored only 18.3% , since it didn’t focus on Python). The Python-optimized 16B model also got around 35% pass@1 on MBPP . These were respectable at the time – for comparison, GPT-J-6B was around ~11% on HumanEval. CodeGen 16B-Mono nearly reached the level of OpenAI’s Codex 12B on HumanEval (which was ~37%). However, newer models in 2023 have far surpassed these numbers. CodeGen’s smaller models (6B, 2B) had proportionally lower scores (e.g. 6B ~13% HumanEval in the paper). In summary, CodeGen 16B was competitive with early Codex and other 2022 models but is now mid-pack.
	•	Training Data: CodeGen was trained on a large corpus of source code combined with some natural language. Specifically, it used the Pile (an 800GB general text dataset) plus GitHub code. The 16B Multi model was trained on 1.2T tokens of which ~50% was code from multiple languages and ~50% was natural text  . The Mono variant was a further fine-tune on just Python code (they took the 16B model and trained a bit more on 180B tokens of Python code only, which improved Python tasks). The code data was permissively licensed (similar to The Stack v1, likely). Languages included Python, Java, JavaScript, etc., but Python was dominant.
	•	Fine-Tuning Details: After the base pretraining (which already included a lot of code), Salesforce released specialized checkpoints: codegen-16B-mono (Python-only as mentioned) and also codegen-16B-nl (natural language) which was more text-tuned. The Python Mono was the most relevant for code generation. This two-step training (general then mono) yielded better results on Python benchmarks . There was no RLHF or instruction tuning in the initial release, so CodeGen models are best used with a prompt that includes some context or function signature to guide the output (they won’t follow arbitrary human instructions out-of-the-box as well as instruct-tuned models do).
	•	Code-Specific Features: CodeGen is a plain causal LM (no special infill capability or conversation fine-tune). Its strengths include generating syntactically correct code for typical tasks and it learned some common libraries/API calls from training. One notable feature: CodeGen was fine-tuned on the HumanEval dataset itself in one experiment  – this was done to show the maximum performance it could achieve (though that fine-tuned model is overfit to HumanEval and not generally useful). This demonstrated that CodeGen can memorize and optimize for specific tasks if needed. In practice, users should use the non-fine-tuned model for real coding tasks to avoid overfitting. CodeGen supports multiple programming languages (the multi model can generate in Java, C++, etc., though with lower proficiency than Python).
	•	Last Updated: The project was released in March 2022. There have been no official new versions since then (Salesforce moved on to CodeT5+ and other research). CodeGen was integrated into NVIDIA’s NeMo framework as a permissive model for code (NVIDIA NGC hosts it as well). It remains an important baseline in academic studies.
	•	Source URLs: GitHub: salesforce/CodeGen and HuggingFace model ids like Salesforce/codegen-16B-mono. The associated paper from Salesforce gives detailed training info. A Deepchecks blog post also summarized CodeGen’s features  .
	•	Deployment Requirements: The 16B model needs ~32GB GPU memory for FP16 inference. It can be run on a single 32GB GPU or two 16GB GPUs (using transformers with device_map). Because it’s an older model, it’s not as optimized as newer architectures, but it’s compatible with most frameworks. The smaller sizes (2.7B, 6B) can run on 8–16GB cards easily. CodeGen models have also been quantized to 4-bit and 8-bit in the community, reducing memory needs significantly (16B can drop to ~8GB in 4-bit).
	•	Notes: CodeGen was one of the first high-performance open code models, and it demonstrated that with sufficient data and scale, open models can rival proprietary ones. However, it is not instruction-tuned – using it in a chatbot or helper scenario may require prefacing the prompt with something like a docstring or function signature to nudge it. For example, providing """Function to X...""" def function_x(...): will lead it to generate the function body. If asked in plain English without such cues, it might not respond as desired. CodeGen opened the door for later models by using an open license and releasing weights, which was hugely beneficial for the community .

InCoder (Meta AI)
	•	Versions & Parameters: InCoder is a 6.7B parameter generative model for code (with a smaller 1.3B version also available) released by Meta AI in 2022. It is unique in that it is a unified model for both left-to-right code generation and infilling (code insertion). InCoder-6.7B served as a proof-of-concept that a single LM can handle editing tasks by predicting masked spans.
	•	License: The model weights were released under a non-commercial license (Creative Commons CC BY-NC 4.0) . This means it can be used and modified freely for research or personal use, but not for commercial products. The training data was all permissively licensed code, but Meta kept the model under CC-NC terms (similar to the original LLaMA release).
	•	Context Window: 2048 tokens (it was based on a GPT architecture with standard context length). However, InCoder’s special training allowed it to use that context in non-sequential ways (with masks for infill positions). Still, you cannot input extremely long files; it’s suited for functions or maybe small files.
	•	Benchmark Scores: InCoder’s raw code generation performance on benchmarks like HumanEval was moderate. The 6.7B model achieves around 18.0% pass@1 on HumanEval (reported in the paper) – roughly on par with GPT-J-6B and far below larger models. It was a bit lower than CodeGen-6B on Python. Where InCoder shines is on infilling tasks: for example, it outperformed other models of similar size on the HumanEval infilling benchmark (where it’s given a partial code with a hole to fill)  . In the MultiPL-E multilingual eval, InCoder-6B also did fairly well for its size. Overall, it’s not a top scorer on pass@1, but it provides capabilities others lacked at release.
	•	Training Data: InCoder was trained exclusively on permissively-licensed code from public sources (GitHub/GitLab) plus StackOverflow Q&A. The total was ~159GB of code after filtering, covering 28 programming languages (with a focus on Python (52GB) and JavaScript, but also including C, C++, Java, etc.) . Additionally, ~57GB of StackOverflow data was included , which likely helped it learn to generate code given natural language questions or to comment code. Importantly, all code with non-permissive licenses (GPL, etc.) was excluded , so the training set is a subset of The Stack (similar to what later projects like StarCoder used).
	•	Fine-Tuning Details: InCoder’s novelty is its training objective. Instead of the standard left-to-right only, it uses a causal masking objective (inspired by Google’s “SpanBERT/UL2” etc.). Specifically, during training, random spans of code are masked out and the model is tasked with generating those spans given the surrounding context . This trains it for infilling. It can also be used in normal mode by simply giving it no masked tokens (then it’s standard LM). There wasn’t a separate fine-tuning stage for instructions or chat in the released version. However, the model by design can handle a prompt with <mask> tokens denoting places to fill.
	•	Code-Specific Features: Infilling: InCoder can insert code into the middle of existing code. For example, you can provide: def add(a, b): <mask> and it will fill in the function body. It uses special tokens (like <|mask:0|> and <|mask:1|> etc. in the raw model) to denote multiple blanks. This was a precursor to later fill-in-the-middle capabilities. Also, InCoder was trained on whole files, not just functions, which helps it generate coherent larger snippets and even entire small programs. Its StackOverflow training gives it some ability to generate not just code but also explanations or answers involving code. For instance, it might provide a code block and text explanation, if prompted in that style. Another feature: because it’s a smaller model, it’s relatively fast and lightweight to use for autocompletion in IDEs (some communities integrated InCoder-6B into VSCode for offline code completion).
	•	Last Updated: Released in April 2022 (ICLR 2023 paper). No new versions since; Meta’s focus moved to bigger models (like Code Llama). InCoder remains an interesting research artifact.
	•	Source URLs: Hugging Face (facebook/incoder-6B and incoder-1B) with model card and community discussions . The paper “InCoder: A Generative Model for Code Infilling and Synthesis” (Fried et al. 2022) is on arXiv. A Google Sites page by the authors also provides examples and usage instructions  .
	•	Deployment Requirements: 6.7B parameters – easily runnable on a single GPU. It requires ~14GB of memory in FP16, or about 7GB with 8-bit quantization. This means even a consumer RTX 3090 (24GB) can run it with plenty of headroom, and a 16GB card can manage with int8. The 1.3B model is even lighter (~3GB). InCoder models are supported in Hugging Face Transformers (AutoModel can load them). For infilling usage, one must use the special token <|mask|> in the prompt and the model will replace it with code (the HF model card gives examples).
	•	Notes: InCoder was the first open model to demonstrate infill, which is particularly useful for code editing scenarios (like “fill in the missing code in this file”). Later models like SantaCoder/StarCoder adopted similar techniques. While larger models have surpassed its accuracy, InCoder can still be useful for lightweight applications. One should remember it’s not instruction-tuned – it won’t respond to “Write a program that…” in pure English as directly. But if you format the prompt as a code file with a gap, it will do well. For example, provide some context code above and then a # TODO: implement function followed by the function signature and a <mask> – InCoder will fill that mask nicely. This behavior aligns with how developers actually operate (leave a TODO and fill it). In summary, InCoder pioneered features that are now becoming standard in code LLMs , at the cost of a modest drop in left-to-right generation performance (a trade-off the authors deemed worth it given the versatility).

CodeGeeX (Tsinghua/Zhipu AI)
	•	Versions & Parameters: CodeGeeX is a 13 billion parameter multilingual code generation model developed through a collaboration between Tsinghua University and Zhipu.AI (released in late 2022, with a paper at KDD 2023). There is one main version (13B) which is available as a base model and via API, sometimes referred to as CodeGeeX-13B. No smaller variants were released publicly. It has associated Visual Studio Code and JetBrains IDE extensions that use the model.
	•	License: CodeGeeX’s code, model weights, and data are fully open. The project is under a Creative Commons BY 4.0 License , meaning you can use and redistribute it (even commercially) as long as you give attribution. They explicitly open-sourced the model weights and even the HumanEval-X benchmark. Additionally, the source code for the model and training pipeline was released under Apache-2.0 on GitHub. This openness was notable, as some other academic models kept weights private.
	•	Context Window: CodeGeeX supports 2048 tokens (standard context). It was not explicitly stated if it can go beyond that, so we assume 2K as the trained context length. This is sufficient for most function or small file completions.
	•	Benchmark Scores: CodeGeeX’s claim to fame is its strong multilingual coding ability. According to the authors, it “achieves the highest average performance compared with other open-sourced multilingual code models of similar scale”  at the time. They introduced HumanEval-X, an extension of HumanEval in 4 languages (C++, Java, JavaScript, Go). CodeGeeX outperformed 15B GPT-Neo and 16B CodeGen-multi on those tasks . In Python (HumanEval original), CodeGeeX’s pass@1 was around 22–25% (approximate, from evaluations and the WizardCoder table which shows 22.9% ). This is lower than specialized Python models like StarCoder (33%) or CodeGen-Mono (29%), but CodeGeeX trades some Python performance for broader language coverage. In their user study, 83.4% of users felt CodeGeeX improved their coding efficiency . To summarize: CodeGeeX is competent but not state-of-the-art in Python, yet among the best for multi-language support in an open model (as of 2023).
	•	Training Data: The model was trained on a massive 850 billion tokens of code across 23 programming languages . This data was collected up to June 2022. Languages include Python, C++, Java, JavaScript, Go, PHP, etc. They likely used the Software Heritage archive / GitHub data (similar to The Stack) but curated to include many languages. They mention writing code in each of 23 languages to fine-tune evaluation. The dataset presumably filtered for permissive licenses (the paper states the model is open and data is shareable, implying no non-permissive contamination). This 850B tokens is about 3-4x the amount CodeGen used, giving CodeGeeX a lot of examples in each language.
	•	Fine-Tuning Details: It’s primarily a pretrained model (decoder-only) without specialized instruction tuning for conversation. The focus was on pretraining on raw code. However, CodeGeeX was later provided as part of coding assistant plugins which likely wrap it with some prompt engineering. The paper doesn’t mention RLHF or instruction tuning; it’s basically a raw LM for code completion. They did, however, fine-tune or evaluate on HumanEval-X by hand-crafting solutions for other languages to create a benchmark . So CodeGeeX itself was not instruction-tuned – if you prompt it in natural language, it might not respond as well as when given code context. Typically, you’d use it by giving a partial code (like writing a comment and a function signature, and then letting it generate the function).
	•	Code-Specific Features: Multilingual support is the standout feature. CodeGeeX can generate code in a variety of languages beyond Python. For example, you could prompt it with a C++ function signature and expect it to generate a plausible implementation. It understands multiple programming paradigms and syntaxes. It also has an IDE integration: the team released extensions for VS Code, JetBrains, etc., which means CodeGeeX can do on-the-fly completions as you type (similar to Copilot). Those extensions likely use a shorter context (maybe last few hundred lines) due to latency, but leverage the model’s knowledge. CodeGeeX was also used to build a code translation tool (since it knows many languages, one can ask it to translate code from say Python to C++ by prompt). According to their report, many users actively used its Cloud Studio integration, generating billions of tokens per week . One limitation: it doesn’t have an infill mechanism like InCoder; it’s left-to-right. Also, it might not have been heavily trained on natural language, so asking it a question in English might yield a code-like or less fluent answer – the expected usage is give it code context.
	•	Last Updated: CodeGeeX was open-sourced in September 2022 (weights on GitHub/HuggingFace). The KDD paper came out in 2023. There’s no v2 announced publicly yet. Zhipu.AI has since worked on a different series (GLM-130B etc.), but CodeGeeX remains their main code model.
	•	Source URLs: GitHub: THUDM/CodeGeeX contains code, model weights, and the VSCode extension. The arXiv/ACM paper “CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X”  . Hugging Face might have a copy (some HF users uploaded CodeGeeX-13B under various names). Official model card info can be found in project’s README.
	•	Deployment Requirements: As a 13B model, it needs roughly ~26GB GPU memory in fp16. With int8 quantization, it can fit on a single 16GB GPU. The inference code for the VSCode extension likely does some optimizations for latency. For most uses, you’d run CodeGeeX on a server with a 32GB GPU or use DeepSpeed quantization on smaller hardware. It doesn’t require special handling beyond the usual for a Transformer LM. Since it’s open and under CC BY, one could also host it in cloud instances or fine-tune it further if needed.
	•	Notes: CodeGeeX filled an important niche by focusing on multilingual code generation. Many other models were Python-centric. If your project involves generating code in less common languages (say Rust, or MATLAB, or Kotlin etc.), CodeGeeX might have some knowledge of those from its wide training set. The open IDE plugins for CodeGeeX gave developers a free Copilot-like experience (with the trade-off of somewhat lower quality than Copilot GPT-3.5). It’s a good example of academia + industry collaboration yielding a usable tool. When using it, keep in mind it’s not as “chatty” – you often have to prompt it with code or at least an initial comment like # write a function to do X followed by the function definition line, to get the best results.

PolyCoder (CMU)
	•	Versions & Parameters: PolyCoder is a 2.7 billion parameter code model released in early 2022 by researchers at Carnegie Mellon University. It also had smaller variants (400M and 160M) but the 2.7B model is the one referred to as PolyCoder. It is based on the GPT-2 architecture (decoder-only Transformer with 32 layers, d_model=2560) . PolyCoder was one of the first open-source code-specific LLMs.
	•	License: The PolyCoder model is released under the MIT License , a very permissive open-source license. All its checkpoint files, tokenizer, and scripts are MIT licensed, meaning it can be used in commercial products freely . This was notable because it allowed organizations to experiment with code generation without legal barriers (unlike OpenAI Codex which was API-only).
	•	Context Window: 2048 tokens, following GPT-2 standards. This allows for generating reasonably sized functions or classes, but not entire long files in one go.
	•	Benchmark Scores: When it was released, PolyCoder’s creators evaluated it on code completion tasks. It performed better than GPT-neo 2.7B in several programming languages (like C, JavaScript, Rust, Scala) . Notably, in the C language, PolyCoder outperformed OpenAI’s Codex (12B) on some benchmarks  – a surprising result attributed to possibly more C code in training or different tokenization. However, on Python (HumanEval) PolyCoder was weaker than Codex. Its pass@1 on HumanEval was reported around ~15% (compared to Codex’s 37%). A later study indicated PolyCoder 2.7B’s exact HumanEval pass@1 was ~4% (which might be using a stricter evaluation or without any few-shot prompting) . In general, PolyCoder was not state-of-the-art in Python, but it was a decent multi-language code model for its size in 2022. Its strength in C was highlighted as beating Codex’s success rate . Keep in mind this is at 2.7B params – much smaller than models like CodeGen-16B or StarCoder, so one wouldn’t expect it to compete with those larger ones on raw performance.
	•	Training Data: PolyCoder was trained on 249 GB of code across 12 programming languages  . The dataset was collected by crawling the top GitHub repositories for each language (with ≥50 stars) – about 25K repos per language, then filtering. The languages include C, C++, Java, JavaScript, Python, Go, Rust, Ruby, PHP, etc. After deduplication and removing license-restricted code, they ended up with ~631 GB raw which was reduced to 249 GB of “clean” code data . They also included some StackOverflow data for niche languages. Importantly, all code was permissively licensed (MIT, Apache, BSD, etc.) or public domain, to allow MIT licensing of the model. This approach ensured no GPL contamination. The training budget was limited: they trained the 2.7B model for 150K steps with a batch of 262K tokens (effectively seeing ~39B tokens) . This is much less than later models (which train on hundreds of billions or trillions of tokens), partly explaining why PolyCoder didn’t reach the same performance levels.
	•	Fine-Tuning Details: PolyCoder was not instruction-tuned or fine-tuned on specific tasks – it’s a pure pretrained LM. They did train three sizes from scratch (2.7B, 400M, 160M) to compare scaling effects . No additional fine-tune on HumanEval was done (to maintain a fair eval, unlike CodeGen’s experiment of fine-tuning on HumanEval which was just for demonstration). Because it wasn’t tuned with human instructions, to use PolyCoder effectively, you provide code context or a docstring prompt rather than asking in plain English. For example, starting with a comment “// function to reverse a string” and then writing function reverseString(str) { and letting it complete would be a good way.
	•	Code-Specific Features: PolyCoder is basically a GPT-2 model for code – it doesn’t have fancy features like fill-in-the-middle or a special tokenizer for code (it uses a standard BPE). However, one advantage of its small size is speed and memory efficiency. It can be deployed on a single GPU or even CPU for lightweight uses. The authors specifically positioned it as an open alternative to Codex for research, allowing exploration of model internals (weights and attention) which isn’t possible with a closed model . They even discussed possibilities like model distillation and retrieval-augmentation on top of PolyCoder, given access to weights . From a user perspective, PolyCoder can autocomplete code in multiple languages reasonably well when prompted with typical code comment cues. It was found to produce particularly secure C code relative to Codex in some analysis (possibly because Codex might have learned insecure patterns from GitHub whereas PolyCoder’s training data was different) – this was mentioned but would need careful validation.
	•	Last Updated: Released January 2022 (the arXiv preprint and model release happened then). No updates since – the authors published it and provided a guide on usage , but did not scale it further (likely due to resource constraints at the time). It remains as-is for historical and research interest.
	•	Source URLs: GitHub repo by Frank Xu et al., and a HuggingFace model under PolyCoder-2.7B (some community conversions exist). The medium article and Reddit threads discussed it as “open-source Codex”  . The paper “A Systematic Evaluation of Large Language Models of Code” (which PolyCoder was a part of) provides evaluation details. PolyCoder is also listed in many surveys of open AI coding tools.
	•	Deployment Requirements: 2.7B params – this is small enough to run on CPU or a single GPU easily. It needs about ~5.5GB of RAM in FP16. On CPU, using an optimized transformer library, it can generate a few tokens per second, which might be acceptable for some use cases. On GPU, it’s very fast. It’s feasible to run PolyCoder on a laptop with enough RAM using int8 quantization. This makes it accessible for offline local use without high-end hardware. Given its MIT license, it could even be embedded into IDE plugins or products freely.
	•	Notes: PolyCoder holds the distinction of being the first truly open code LLM of significant size. It demonstrated the viability of open models in this domain and provided a baseline for later research to improve upon. While larger models have eclipsed it, PolyCoder is still cited as an example in discussions about licensing (it proved you can train on only permissive code and release under MIT). For educational purposes or experiments in model fine-tuning, PolyCoder is a nice starting point due to its small size. One should temper expectations for solving complex coding tasks – it may do okay for simple algorithms or boilerplate, but struggles with more complex logic (especially in Python where it’s weaker than contemporary models). Nonetheless, it’s an important part of the evolution of code LLMs, illustrating the trade-offs of model size and training data quality. It also showed that on certain languages like C, a smaller focused model can sometimes outperform a general large model , an observation that later inspired ideas like specializing models or using retrieval augmentation for specific domains.

Phi-1 (Microsoft, Small Models for Code)
	•	Versions & Parameters: Phi-1 is a 1.3 billion parameter Transformer model released by Microsoft in mid-2023 as part of their “Phi” series of small language models (SLMs). It is specialized for Python code generation. There is also a Phi-1.5 (1.3B + retrieval augmentation) and smaller Phi models (350M, etc.), but Phi-1 (1.3B, standalone) is the primary open checkpoint.
	•	License: The model is released under the MIT License , which is highly permissive and allows commercial use. Microsoft provided not just the weights but also a detailed cookbook on training these small models, all under open licenses. This stands in contrast to many big-tech models which often carry non-commercial clauses – here, Microsoft explicitly made Phi-1 free for any use.
	•	Context Window: Phi-1 has an 2048-token context length (in line with its GPT-2/3 style architecture). This is enough for typical coding interview problems or single-file scripts. It does not have extended context or special positioning features.
	•	Benchmark Scores: Despite its small size, Phi-1 demonstrated surprisingly strong results on basic coding tasks. It achieved over 50% pass@1 on HumanEval (164 simple Python problems) in zero-shot evaluation . This is an impressive accuracy for a 1.3B model – by comparison, GPT-3.5 (175B) was around 48% on an earlier HumanEval version, although GPT-3.5 has improved since. Phi-1’s >50% suggests it outperforms many larger open models on those simple tasks, thanks to targeted training. It is not as good on more complex or diverse tasks, but for basic function writing, it’s quite reliable. On MBPP (which involves slightly more complex prompts), Phi-1 would likely be lower (the paper cites ~30-40% on MBPP, though not explicitly in the card). The key takeaway: Phi-1 offered “competitive coding ability with far fewer parameters”. Subsequent Phi-1.5 (which adds a retrieval mechanism) pushed HumanEval to ~63% as per Microsoft’s blog, but that uses an external knowledge base and isn’t a pure LM.
	•	Training Data: Phi-1’s training involved a mixture of sources: it used subsets of The Stack v1.2 (permissive code) for raw code, plus Q&A from StackOverflow, plus “textbook-style” synthetic coding exercises generated by GPT-3.5 . In essence, Microsoft curated a training set that focuses on Python coding skills. They likely limited the variety of tasks so the model can specialize. The training approach “Textbooks Are All You Need” (as referenced in an associated paper) suggests they generated a lot of high-quality Python problem-and-solution pairs (simulating a programming textbook) using larger models, and then trained Phi-1 on those. This focused training is what allows a small model to do so well on evaluation – it hasn’t wasted capacity on unrelated data. Total tokens used were not disclosed in the card, but presumably on the order of tens of billions (not trillions).
	•	Fine-Tuning Details: Phi-1 was trained from scratch (or possibly from a small pre-trained base) on the code and Q&A mix. It is effectively instruction-tuned during training because a lot of its data is in Q&A or problem-solution format. No separate RLHF or post-training fine-tune was mentioned – it’s more like they baked the instruction-following into the training by using the right formats (like prompting GPT-3.5 to generate question-answer pairs, then feeding those to Phi-1). The model card also notes that it’s specialized and not a general chatbot – it may appear to answer natural questions (because of StackOverflow data), but it’s primarily a coding bot . So one should feed it prompts related to coding. Phi-1.5 (not fully covered here) added a retrieval component where the model can fetch relevant code snippets from a database during generation – that further improves accuracy especially for library usage, but it’s a separate system (and also open source).
	•	Code-Specific Features: Phi-1 is tuned just for coding tasks. It expects the format of a function or code file and will continue it. For example, if given a prompt like:

def is_prime(n):
    """
    Determine if n is a prime number.
    """

it will generate the rest of the function correctly with >50% probability on such straightforward prompts. It’s not meant to hold a conversation or do non-coding NLP. The model was trained on certain allowed Python libraries (they mention it mostly saw typing, math, random, collections, datetime, itertools in fine-tuning) , so it’s very good with those but might be clueless about others (and will make things up outside its narrow scope). Another feature: because it’s small, it’s fast and lightweight, potentially usable on mobile devices or browsers with WebGPU in the future. Microsoft’s aim with Phi was to produce models that can run in low-resource settings yet still be helpful for coding. Phi-1’s limitations include occasionally replicating code seen online (due to training on GitHub data, though they filtered a lot) and generating incorrect code when faced with uncommon tasks . They openly list these limitations and biases in the card.

	•	Last Updated: Phi-1 was released in June 2023. The Phi series is ongoing (Phi-2, Phi-3 etc. presumably in research). Phi-1.5 (with retrieval) came later in 2023. It’s likely Microsoft will continue exploring this small-model approach, but for now Phi-1 is static.
	•	Source URLs: Hugging Face: microsoft/phi-1 (with a detailed model card) . The paper/blog titled “Small Models Can Code: Phi-1” and the Azure AI blog about Phi models give additional context. The project’s GitHub (PhiLab) provides code for training and the “Phi CookBook” for usage.
	•	Deployment Requirements: 1.3B parameters – extremely deployable. ~2.6 GB in fp16, ~1.3 GB in 8-bit. It can run on a laptop GPU or even CPU (with GPTQ 4-bit it could be under 1GB, which some Raspberry Pi-level devices could load albeit slowly). Microsoft’s intention is that such models could even run in-browser or on edge devices eventually. For now, using a standard GPU yields very fast inference (1.3B is ~1/25th the size of GPT-3, so it’s quick). This means one can integrate Phi-1 into editors without needing cloud services, which is a big plus for privacy and cost.
	•	Notes: Phi-1 is an exciting development because it challenges the notion that “bigger is always better.” By focusing on quality of training data and scope of expertise, it achieves high accuracy on specific tasks. It shows that for applications like coding, a carefully trained 1.3B model can outperform some generic 6B or even 13B models that weren’t as finely tuned . The trade-off is narrower versatility – Phi-1 won’t help you write an essay or have a conversation about general topics. But if your need is a lightweight code helper that can generate correct solutions to competitive-programming-style questions or basic algorithm implementations, Phi-1 is a compelling choice. It also opens the door to community enhancements: people might take Phi-1 and fine-tune it on other languages or tasks, given its open license. In short, Phi-1 demonstrates “right training beats sheer scale” for targeted domains and provides a foundation for practical, private coding assistants.