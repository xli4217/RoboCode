model_name,version,parameter_count,license,context_window,benchmark_scores,training_data,fine_tuning_details,code_specific_features,last_updated,source_url,deployment_requirements,notes
CodeLlama,7B/13B/34B/70B + Python/Instruct,7B–70B,Meta LLAMA 2 License,16K (100K extended),"HumanEval up to 67%, MBPP ~65%",500B tokens permissive GitHub code; Python emphasis,Multi-stage: code fine-tune + Python specialization + instruct,"Fill-in-the-middle (7B/13B/70B), long-context",Aug 2023,https://huggingface.co/meta-llama,70B: ~80GB GPU; 34B: ~40GB; 7B/13B: 1×24GB w/8-bit,"High accuracy, Python strong, Instruct safer for prod use"
StarCoder2,3B/7B/15B,3B–15B,BigCode Open RAIL-M,16K,"HumanEval ~50–60% (15B), MBPP ~50%","Stack v2 (900B tokens, 600+ languages)","Instruction-tuned (Instruct v0.1), FIM training","Fill-in-the-middle, multi-language (600+)",Feb 2024,https://huggingface.co/bigcode,15B: ~32GB GPU FP16; 3B: ~7GB; 7B: ~14GB,Strong multilingual coverage; open analog of Codex
WizardCoder,"V1.0 (15B), V1.1 (33B)",15B/33B,"15B: OpenRAIL-M, 33B: MSR non-commercial","8K (15B), 16K (33B)","HumanEval 79.9% (33B), 57.3% (15B)",StarCoder/DeepSeek bases + 78K Evol-Instruct data,Evol-Instruct fine-tuning pipeline,"Instruction-following, bilingual (EN/zh) at 33B",Jan 2024,https://huggingface.co/WizardLM,"33B: ~65GB (2×40GB GPUs), 15B: ~30GB",33B variant non-commercial; Python variant 34B under LLaMA2 license
DeepSeek Coder,"V1 (1.3B–33B), V2 (16B MoE, 236B MoE)",1.3B–33B; V2: 236B (21B active),DeepSeek License (commercial allowed),"V1: 16K, V2: 128K",V1-33B HumanEval 78.7%; V2 ≈ GPT-4 Turbo,"2T tokens (87% code, 13% EN/zh text); V2 +6T tokens, 338 languages",Instruct tuned on 2B tokens Q&A; MoE routing,"Infilling, bilingual EN/zh, project-level completion",Jan 2024 (V2),https://huggingface.co/deepseek-ai,33B: ~60GB; V2-236B: 8×80GB GPUs,"Bilingual, strong open alternative to GPT-4-level coding"
CodeT5+,"770M, 2B, 6B, 16B",770M–16B,MIT (Salesforce),~1K (seq2seq),HumanEval 35% (16B Instruct),"Permissive GitHub code + docstrings, 8–10 languages",Instruction tuned with CodeAlpaca-style data,"Encoder–decoder; summarization, translation",May 2023,https://huggingface.co/Salesforce,16B: ~32GB GPU; 2B: ~4GB,"Versatile, strong for code understanding + generation"
CodeGen,"350M, 2.7B, 6B, 16B",350M–16B,Apache-2.0,2K,HumanEval 29.3% (16B Mono),Pile + GitHub code (multi vs Python-only),Mono Python fine-tune improved accuracy,"Multi-language, Python specialized variants",Mar 2022,https://huggingface.co/Salesforce,16B: ~32GB GPU FP16,"First major open dense model for code, Apache license"
InCoder,"1.3B, 6.7B",1.3B/6.7B,CC BY-NC 4.0 (non-commercial),2K,HumanEval ~18% (6.7B),159GB permissive code (28 languages) + 57GB StackOverflow,Span-infilling objective training,"Infilling (<mask> tokens), code+doc answers",Apr 2022,https://huggingface.co/facebook/incoder-6B,6.7B: ~14GB FP16,First open infill model; weaker generation
CodeGeeX,13B,13B,CC BY 4.0,2K,HumanEval ~22–25%; multilingual SOTA at release,850B tokens code across 23 languages,Pretrained only; IDE plugin integration,"Multilingual codegen (23 langs), IDE plugins",Sep 2022,https://github.com/THUDM/CodeGeeX,13B: ~26GB GPU,Multilingual emphasis; strong C++/JavaScript support
PolyCoder,2.7B,2.7B,MIT,2K,HumanEval ~15%; strong C performance,249GB permissive code (12 languages),"Trained from scratch, no instruct","Strong on C, simple design",Jan 2022,https://huggingface.co/PolyCoder-2.7B,~5.5GB GPU,"First open MIT licensed code LLM, small scale"
Phi-1,1.3B,1.3B,MIT,2K,HumanEval >50%,Python code + StackOverflow + synthetic textbooks,"Targeted Q&A training, no RLHF","Small, efficient, Python-focused",Jun 2023,https://huggingface.co/microsoft/phi-1,1.3B: ~2.6GB GPU,Tiny but highly efficient for Python
